{
    "module-tokenize.html": {
        "modified": "2007-04-18 14:20:24",
        "size": "10.35KB",
        "md5": "c0dad2385dcd1dd3e7e66e6bc252fdc6",
        "sha256": "7e058b5fed94e78d428c280eb48b7fa2faa22e43a54769adb05fa3c66855efb9"
    },
    "Metadata": {
        "Content-Encoding": "UTF-8",
        "Content-Type": "text/html; charset=UTF-8",
        "Content-Type-Hint": "text/html; charset=utf-8",
        "X-Parsed-By": [
            "org.apache.tika.parser.DefaultParser",
            "org.apache.tika.parser.html.HtmlParser"
        ],
        "X-TIKA:content_handler": "ToTextContentHandler",
        "X-TIKA:embedded_depth": "0",
        "X-TIKA:parse_time_millis": "1",
        "aesop": "information",
        "dc:title": "30.5 tokenize -- Tokenizer for Python source",
        "resourceName": "b'module-tokenize.html'",
        "title": "30.5 tokenize -- Tokenizer for Python source"
    }
}